{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face JS libraries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [@huggingface/hub](https://huggingface.co/docs/huggingface.js/hub/README): Interact with huggingface.co to create or delete repos and commit / download files\n",
    "* [@huggingface/inference](https://huggingface.co/docs/huggingface.js/inference/README): Use the Inference API to make calls to 100,000+ Machine Learning models, or your own inference endpoints!\n",
    "* [@xenova/transformers](https://huggingface.co/docs/transformers.js/index): State-of-the-art Machine Learning for the web. Run 🤗 Transformers directly in your browser, with no need for a server!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Loading (API)\n",
    "\n",
    "The library runs on Node.js and in browser environments and is available on npm [@huggingface/inference](https://www.npmjs.com/package/@huggingface/inference). To load it in a browser/notebook, you can use ES modules via [jsdeliver.net](jsdelivr.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "javascript"
    },
    "polyglot_notebook": {
     "kernelName": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "#!set --name apiKey --value @password:\"Please enter your api key:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "javascript"
    },
    "polyglot_notebook": {
     "kernelName": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "HFInference = (await import('https://cdn.jsdelivr.net/npm/@huggingface/inference@2.5.1/+esm')).HfInference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "javascript"
    },
    "polyglot_notebook": {
     "kernelName": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "hf = new HFInference(apiKey) // TODO: Add api-key to shell variables and access from notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "dotnet_interactive": {
     "language": "javascript"
    },
    "polyglot_notebook": {
     "kernelName": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "textGenIn = \"Where is Paris?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "dotnet_interactive": {
     "language": "javascript"
    },
    "polyglot_notebook": {
     "kernelName": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "textGenRes = await hf.textGeneration({\n",
    "  model: \"EleutherAI/pythia-70m-deduped\",\n",
    "  inputs: textGenIn\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "dotnet_interactive": {
     "language": "javascript"
    },
    "polyglot_notebook": {
     "kernelName": "javascript"
    }
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "generated_text": "Where is Paris?\n\nI am not a Frenchman, but I am a Frenchman."
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console.log(textGenRes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Loading (\"Local\")\n",
    "\n",
    "The library runs on Node.js and in browser environments and is available on npm [@xenova/transformers](https://www.npmjs.com/package/@xenova/transformers). To load it in a browser/notebook, you can use ES modules via [jsdeliver.net](jsdelivr.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "dotnet_interactive": {
     "language": "javascript"
    },
    "polyglot_notebook": {
     "kernelName": "javascript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "javascript"
    },
    "polyglot_notebook": {
     "kernelName": "javascript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "dotnet_interactive": {
     "language": "javascript"
    },
    "polyglot_notebook": {
     "kernelName": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "transformers = await import('https://cdn.jsdelivr.net/npm/@xenova/transformers@2.2.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "dotnet_interactive": {
     "language": "javascript"
    },
    "polyglot_notebook": {
     "kernelName": "javascript"
    }
   },
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Unexpected token 'U', \"Unauthorized\" is not valid JSON",
     "output_type": "error",
     "traceback": [
      "Unexpected token 'U', \"Unauthorized\" is not valid JSON"
     ]
    }
   ],
   "source": [
    "model = await transformers.AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m-deduped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "dotnet_interactive": {
     "language": "javascript"
    },
    "polyglot_notebook": {
     "kernelName": "javascript"
    }
   },
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Unexpected token 'U', \"Unauthorized\" is not valid JSON",
     "output_type": "error",
     "traceback": [
      "Unexpected token 'U', \"Unauthorized\" is not valid JSON"
     ]
    }
   ],
   "source": [
    "extractor = await transformers.pipeline(\n",
    "  \"feature-extraction\",\n",
    "  \"Xenova/all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "dotnet_interactive": {
     "language": "javascript"
    },
    "polyglot_notebook": {
     "kernelName": "javascript"
    }
   },
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Unexpected token 'U', \"Unauthorized\" is not valid JSON",
     "output_type": "error",
     "traceback": [
      "Unexpected token 'U', \"Unauthorized\" is not valid JSON"
     ]
    }
   ],
   "source": [
    "console.log(await transformers.AutoConfig.from_pretrained('bert-base-uncased'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "dotnet_interactive": {
     "language": "javascript"
    },
    "polyglot_notebook": {
     "kernelName": "javascript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No model specified. Using default model: \"Xenova/distilbert-base-uncased-finetuned-sst-2-english\"."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "Error",
     "evalue": "Unexpected token 'U', \"Unauthorized\" is not valid JSON",
     "output_type": "error",
     "traceback": [
      "Unexpected token 'U', \"Unauthorized\" is not valid JSON"
     ]
    }
   ],
   "source": [
    "console.log(await transformers.pipeline('sentiment-analysis'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Notes\n",
    "\n",
    "\n",
    "## Models in Action\n",
    "\n",
    "### Real-time video editing\n",
    " * https://app.runwayml.com\n",
    " * https://twitter.com/StabilityAI_JP\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "dotnet_interactive": {
     "language": "javascript"
    },
    "polyglot_notebook": {
     "kernelName": "javascript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "---\n",
       "language:\n",
       "- en\n",
       "tags:\n",
       "- pytorch\n",
       "- causal-lm\n",
       "- pythia\n",
       "license: apache-2.0\n",
       "datasets:\n",
       "- EleutherAI/the_pile_deduplicated\n",
       "---\n",
       "\n",
       "The *Pythia Scaling Suite* is a collection of models developed to facilitate \n",
       "interpretability research [(see paper)](https://arxiv.org/pdf/2304.01373.pdf). \n",
       "It contains two sets of eight models of sizes \n",
       "70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B. For each size, there are two \n",
       "models: one trained on the Pile, and one trained on the Pile after the dataset \n",
       "has been globally deduplicated. All 8 model sizes are trained on the exact \n",
       "same data, in the exact same order. We also provide 154 intermediate \n",
       "checkpoints per model, hosted on Hugging Face as branches.\n",
       "\n",
       "The Pythia model suite was designed to promote scientific \n",
       "research on large language models, especially interpretability research. \n",
       "Despite not centering downstream performance as a design goal, we find the \n",
       "models <a href=\"#evaluations\">match or exceed</a> the performance of \n",
       "similar and same-sized models, such as those in the OPT and GPT-Neo suites.\n",
       "\n",
       "<details>\n",
       "  <summary style=\"font-weight:600\">Details on previous early release and naming convention.</summary>\n",
       "\n",
       "Previously, we released an early version of the Pythia suite to the public. \n",
       "However, we decided to retrain the model suite to address a few hyperparameter \n",
       "discrepancies. This model card <a href=\"#changelog\">lists the changes</a>; \n",
       "see appendix B in the Pythia paper for further discussion. We found no \n",
       "difference in benchmark performance between the two Pythia versions. \n",
       "The old models are \n",
       "[still available](https://huggingface.co/models?other=pythia_v0), but we \n",
       "suggest the retrained suite if you are just starting to use Pythia.<br>\n",
       "**This is the current release.**\n",
       "\n",
       "Please note that all models in the *Pythia* suite were renamed in January \n",
       "2023. For clarity, a <a href=\"#naming-convention-and-parameter-count\">table \n",
       "comparing the old and new names</a> is provided in this model card, together \n",
       "with exact parameter counts.\n",
       "</details>\n",
       "<br>\n",
       "\n",
       "# Pythia-70M-deduped\n",
       "\n",
       "## Model Details\n",
       "\n",
       "- Developed by: [EleutherAI](http://eleuther.ai)\n",
       "- Model type: Transformer-based Language Model\n",
       "- Language: English\n",
       "- Learn more: [Pythia's GitHub repository](https://github.com/EleutherAI/pythia)\n",
       " for training procedure, config files, and details on how to use.\n",
       " [See paper](https://arxiv.org/pdf/2304.01373.pdf) for more evals and implementation\n",
       " details.\n",
       "- Library: [GPT-NeoX](https://github.com/EleutherAI/gpt-neox)\n",
       "- License: Apache 2.0\n",
       "- Contact: to ask questions about this model, join the [EleutherAI \n",
       "Discord](https://discord.gg/zBGx3azzUn), and post them in `#release-discussion`.\n",
       " Please read the existing *Pythia* documentation before asking about it in the \n",
       " EleutherAI Discord. For general correspondence: [contact@eleuther.\n",
       " ai](mailto:contact@eleuther.ai).\n",
       "\n",
       "<figure>\n",
       "\n",
       "| Pythia model | Non-Embedding Params | Layers | Model Dim | Heads | Batch Size | Learning Rate         | Equivalent Models      |\n",
       "| -----------: | -------------------: | :----: | :-------: | :---: | :--------: | :-------------------: | :--------------------: |\n",
       "| 70M          | 18,915,328           | 6      | 512       | 8     | 2M         | 1.0 x 10<sup>-3</sup> | —                      |\n",
       "| 160M         | 85,056,000           | 12     | 768       | 12    | 2M         | 6.0 x 10<sup>-4</sup> | GPT-Neo 125M, OPT-125M |\n",
       "| 410M         | 302,311,424          | 24     | 1024      | 16    | 2M         | 3.0 x 10<sup>-4</sup> | OPT-350M               |\n",
       "| 1.0B         | 805,736,448          | 16     | 2048      | 8     | 2M         | 3.0 x 10<sup>-4</sup> | —                      |\n",
       "| 1.4B         | 1,208,602,624        | 24     | 2048      | 16    | 2M         | 2.0 x 10<sup>-4</sup> | GPT-Neo 1.3B, OPT-1.3B |\n",
       "| 2.8B         | 2,517,652,480        | 32     | 2560      | 32    | 2M         | 1.6 x 10<sup>-4</sup> | GPT-Neo 2.7B, OPT-2.7B |\n",
       "| 6.9B         | 6,444,163,072        | 32     | 4096      | 32    | 2M         | 1.2 x 10<sup>-4</sup> | OPT-6.7B               |\n",
       "| 12B          | 11,327,027,200       | 36     | 5120      | 40    | 2M         | 1.2 x 10<sup>-4</sup> | —                      |\n",
       "<figcaption>Engineering details for the <i>Pythia Suite</i>. Deduped and \n",
       "non-deduped models of a given size have the same hyperparameters. “Equivalent” \n",
       "models have <b>exactly</b> the same architecture, and the same number of \n",
       "non-embedding parameters.</figcaption>\n",
       "</figure>\n",
       "\n",
       "## Uses and Limitations\n",
       "\n",
       "### Intended Use\n",
       "\n",
       "The primary intended use of Pythia is research on the behavior, functionality, \n",
       "and limitations of large language models. This suite is intended to provide \n",
       "a controlled setting for performing scientific experiments. We also provide \n",
       "154 checkpoints per model: initial `step0`, 10 log-spaced checkpoints \n",
       "`step{1,2,4...512}`, and 143 evenly-spaced checkpoints from `step1000` to \n",
       "`step143000`. These checkpoints are hosted on Hugging Face as branches. Note \n",
       "that branch `143000` corresponds exactly to the model checkpoint on the `main` \n",
       "branch of each model.\n",
       "\n",
       "You may also further fine-tune and adapt Pythia-70M-deduped for deployment, \n",
       "as long as your use is in accordance with the Apache 2.0 license. Pythia \n",
       "models work with the Hugging Face [Transformers \n",
       "Library](https://huggingface.co/docs/transformers/index). If you decide to use \n",
       "pre-trained Pythia-70M-deduped as a basis for your fine-tuned model, please \n",
       "conduct your own risk and bias assessment. \n",
       "\n",
       "### Out-of-scope use\n",
       "\n",
       "The Pythia Suite is **not** intended for deployment. It is not a in itself \n",
       "a product and cannot be used for human-facing interactions. For example, \n",
       "the model may generate harmful or offensive text. Please evaluate the risks\n",
       "associated with your particular use case.\n",
       "\n",
       "Pythia models are English-language only, and are not suitable for translation \n",
       "or generating text in other languages.\n",
       "\n",
       "Pythia-70M-deduped has not been fine-tuned for downstream contexts in which \n",
       "language models are commonly deployed, such as writing genre prose, \n",
       "or commercial chatbots. This means Pythia-70M-deduped will **not** \n",
       "respond to a given prompt the way a product like ChatGPT does. This is because,\n",
       " unlike this model, ChatGPT was fine-tuned using methods such as Reinforcement \n",
       "Learning from Human Feedback (RLHF) to better “follow” human instructions.\n",
       "\n",
       "### Limitations and biases\n",
       "\n",
       "The core functionality of a large language model is to take a string of text \n",
       "and predict the next token. The token used by the model need not produce the \n",
       "most “accurate” text. Never rely on Pythia-70M-deduped to produce factually accurate \n",
       "output.\n",
       "\n",
       "This model was trained on [the Pile](https://pile.eleuther.ai/), a dataset \n",
       "known to contain profanity and texts that are lewd or otherwise offensive. \n",
       "See [Section 6 of the Pile paper](https://arxiv.org/abs/2101.00027) for a \n",
       "discussion of documented biases with regards to gender, religion, and race. \n",
       "Pythia-70M-deduped may produce socially unacceptable or undesirable text, *even if* \n",
       "the prompt itself does not include anything explicitly offensive. \n",
       "\n",
       "If you plan on using text generated through, for example, the Hosted Inference \n",
       "API, we recommend having a human curate the outputs of this language model \n",
       "before presenting it to other people. Please inform your audience that the \n",
       "text was generated by Pythia-70M-deduped.\n",
       "\n",
       "### Quickstart\n",
       "\n",
       "Pythia models can be loaded and used via the following code, demonstrated here \n",
       "for the third `pythia-70m-deduped` checkpoint:\n",
       "\n",
       "```python\n",
       "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
       "\n",
       "model = GPTNeoXForCausalLM.from_pretrained(\n",
       "  \"EleutherAI/pythia-70m-deduped\",\n",
       "  revision=\"step3000\",\n",
       "  cache_dir=\"./pythia-70m-deduped/step3000\",\n",
       ")\n",
       "\n",
       "tokenizer = AutoTokenizer.from_pretrained(\n",
       "  \"EleutherAI/pythia-70m-deduped\",\n",
       "  revision=\"step3000\",\n",
       "  cache_dir=\"./pythia-70m-deduped/step3000\",\n",
       ")\n",
       "\n",
       "inputs = tokenizer(\"Hello, I am\", return_tensors=\"pt\")\n",
       "tokens = model.generate(**inputs)\n",
       "tokenizer.decode(tokens[0])\n",
       "```\n",
       "\n",
       "Revision/branch `step143000` corresponds exactly to the model checkpoint on \n",
       "the `main` branch of each model.<br>\n",
       "For more information on how to use all Pythia models, see [documentation on \n",
       "GitHub](https://github.com/EleutherAI/pythia).\n",
       "\n",
       "## Training\n",
       "\n",
       "### Training data\n",
       "\n",
       "Pythia-70M-deduped was trained on the Pile **after the dataset has been globally \n",
       "deduplicated**.<br>\n",
       "[The Pile](https://pile.eleuther.ai/) is a 825GiB general-purpose dataset in \n",
       "English. It was created by EleutherAI specifically for training large language \n",
       "models. It contains texts from 22 diverse sources, roughly broken down into \n",
       "five categories: academic writing (e.g. arXiv), internet (e.g. CommonCrawl), \n",
       "prose (e.g. Project Gutenberg), dialogue (e.g. YouTube subtitles), and \n",
       "miscellaneous (e.g. GitHub, Enron Emails). See [the Pile \n",
       "paper](https://arxiv.org/abs/2101.00027) for a breakdown of all data sources, \n",
       "methodology, and a discussion of ethical implications. Consult [the \n",
       "datasheet](https://arxiv.org/abs/2201.07311) for more detailed documentation \n",
       "about the Pile and its component datasets. The Pile can be downloaded from \n",
       "the [official website](https://pile.eleuther.ai/), or from a [community \n",
       "mirror](https://the-eye.eu/public/AI/pile/).\n",
       "\n",
       "### Training procedure\n",
       "\n",
       "All models were trained on the exact same data, in the exact same order. Each \n",
       "model saw 299,892,736,000 tokens during training, and 143 checkpoints for each \n",
       "model are saved every 2,097,152,000 tokens, spaced evenly throughout training, \n",
       "from `step1000` to `step143000` (which is the same as `main`). In addition, we \n",
       "also provide frequent early checkpoints: `step0` and `step{1,2,4...512}`.\n",
       "This corresponds to training for just under 1 epoch on the Pile for \n",
       "non-deduplicated models, and about 1.5 epochs on the deduplicated Pile.\n",
       "\n",
       "All *Pythia* models trained for 143000 steps at a batch size \n",
       "of 2M (2,097,152 tokens).<br>\n",
       "See [GitHub](https://github.com/EleutherAI/pythia) for more details on training\n",
       " procedure, including [how to reproduce \n",
       " it](https://github.com/EleutherAI/pythia/blob/main/README.md#reproducing-training).<br>\n",
       "Pythia uses the same tokenizer as [GPT-NeoX-\n",
       "20B](https://huggingface.co/EleutherAI/gpt-neox-20b).\n",
       "\n",
       "## Evaluations\n",
       "\n",
       "All 16 *Pythia* models were evaluated using the [LM Evaluation \n",
       "Harness](https://github.com/EleutherAI/lm-evaluation-harness). You can access \n",
       "the results by model and step at `results/json/*` in the [GitHub \n",
       "repository](https://github.com/EleutherAI/pythia/tree/main/results/json/).<br>\n",
       "Expand the sections below to see plots of evaluation results for all \n",
       "Pythia and Pythia-deduped models compared with OPT and BLOOM.\n",
       "\n",
       "<details>\n",
       "  <summary>LAMBADA – OpenAI</summary>\n",
       "  <img src=\"/EleutherAI/pythia-12b/resolve/main/eval_plots/lambada_openai_v1.png\" style=\"width:auto\"/>\n",
       "</details>\n",
       "\n",
       "<details>\n",
       "  <summary>Physical Interaction: Question Answering (PIQA)</summary>\n",
       "  <img src=\"/EleutherAI/pythia-12b/resolve/main/eval_plots/piqa_v1.png\" style=\"width:auto\"/>\n",
       "</details>\n",
       "\n",
       "<details>\n",
       "  <summary>WinoGrande</summary>\n",
       "  <img src=\"/EleutherAI/pythia-12b/resolve/main/eval_plots/winogrande_v1.png\" style=\"width:auto\"/>\n",
       "</details>\n",
       "\n",
       "<details>\n",
       "  <summary>AI2 Reasoning Challenge—Easy Set</summary>\n",
       "  <img src=\"/EleutherAI/pythia-12b/resolve/main/eval_plots/arc_easy_v1.png\" style=\"width:auto\"/>\n",
       "</details>\n",
       "\n",
       "<details>\n",
       "  <summary>SciQ</summary>\n",
       "  <img src=\"/EleutherAI/pythia-12b/resolve/main/eval_plots/sciq_v1.png\" style=\"width:auto\"/>\n",
       "</details>\n",
       "\n",
       "## Changelog\n",
       "\n",
       "This section compares differences between previously released \n",
       "[Pythia v0](https://huggingface.co/models?other=pythia_v0) and the current \n",
       "models. See Appendix B of the Pythia paper for further discussion of these \n",
       "changes and the motivation behind them. We found that retraining Pythia had no \n",
       "impact on benchmark performance.\n",
       "\n",
       "- All model sizes are now trained with uniform batch size of 2M tokens. \n",
       "Previously, the models of size 160M, 410M, and 1.4B parameters were trained \n",
       "with batch sizes of 4M tokens.\n",
       "- We added checkpoints at initialization (step 0) and steps {1,2,4,8,16,32,64,\n",
       "128,256,512} in addition to every 1000 training steps.\n",
       "- Flash Attention was used in the new retrained suite.\n",
       "- We remedied a minor inconsistency that existed in the original suite: all \n",
       "models of size 2.8B parameters or smaller had a learning rate (LR) schedule \n",
       "which decayed to a minimum LR of 10% the starting LR rate, but the 6.9B and \n",
       "12B models all used an LR schedule which decayed to a minimum LR of 0. In \n",
       "the redone training runs, we rectified this inconsistency: all models now were \n",
       "trained with LR decaying to a minimum of 0.1× their maximum LR.\n",
       "\n",
       "### Naming convention and parameter count\n",
       "\n",
       "*Pythia* models were renamed in January 2023. It is possible that the old \n",
       "naming convention still persists in some documentation by accident. The \n",
       "current naming convention (70M, 160M, etc.) is based on total parameter count. \n",
       "\n",
       "<figure style=\"width:32em\">\n",
       "  \n",
       "| current Pythia suffix | old suffix | total params   | non-embedding params |\n",
       "| --------------------: | ---------: | -------------: | -------------------: |\n",
       "| 70M                   | 19M        | 70,426,624     | 18,915,328           |\n",
       "| 160M                  | 125M       | 162,322,944    | 85,056,000           |\n",
       "| 410M                  | 350M       | 405,334,016    | 302,311,424          |\n",
       "| 1B                    | 800M       | 1,011,781,632  | 805,736,448          |\n",
       "| 1.4B                  | 1.3B       | 1,414,647,808  | 1,208,602,624        |\n",
       "| 2.8B                  | 2.7B       | 2,775,208,960  | 2,517,652,480        |\n",
       "| 6.9B                  | 6.7B       | 6,857,302,016  | 6,444,163,072        |\n",
       "| 12B                   | 13B        | 11,846,072,320 | 11,327,027,200       |\n",
       "</figure>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Discard\n",
    "huggingFaceHub = await import('https://cdn.jsdelivr.net/npm/@huggingface/hub@0.8.3/+esm')\n",
    "repo = { type: \"model\", name: \"EleutherAI/pythia-70m-deduped\" }\n",
    "console.log(await (await huggingFaceHub.downloadFile({repo, path: \"README.md\"})).text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "javascript"
    },
    "polyglot_notebook": {
     "kernelName": "javascript"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "polyglot-notebook"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     },
     {
      "aliases": [],
      "languageName": "javascript",
      "name": "javascript"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
